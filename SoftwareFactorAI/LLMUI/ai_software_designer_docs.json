[
    {
      "name": "design_recommendation.md",
      "type": "markdown",
      "content": "# Design Recommendation\n\n## Summary\n\nThis document outlines the design for a local web application that interacts with different Language Model (LLM) providers. The application allows users to select an LLM provider and model, define system and user prompts, and display/save the LLM's output. The core technology choice is TypeScript for both frontend and backend to ensure type safety and code reusability.\n\n## Key Factors Considered\n\n*   **Maintainability:** Using TypeScript for both frontend and backend simplifies development and maintenance.\n*   **Extensibility:** The design should easily accommodate new LLM providers and features.\n*   **User Experience:** The interface should be intuitive and easy to use.\n*   **Security:** While primarily local, we'll still follow best practices for input sanitization and API interaction.\n*   **Scalability:** Not a primary concern for a local application, but the design should not hinder future scalability if needed.\n\n## Recommended Approach\n\nWe'll use a client-server architecture. The frontend will be a Single Page Application (SPA) built with TypeScript, using a framework-agnostic approach (vanilla TypeScript with minimal dependencies) for simplicity. The backend will be a lightweight Node.js server (using Express.js for simplicity) also written in TypeScript. Communication between the client and server will be through a RESTful API.\n\nThe choice of framework-agnostic (vanilla JS) app is justified because:\n- The requirements are straight-forward.\n- A framework would add complexity.\n- We want to show the details of the implementation.\n\n## Rejected Alternatives\n\n*   **Python Backend (Flask/Django):** While viable, using TypeScript for both client and server streamlines development and reduces context switching.\n*   **Frontend Frameworks (React, Angular, Vue):** These are overkill for this simple application and would add unnecessary complexity.\n*   **Monolithic Architecture:** Separating client and server allows for independent scaling and development if requirements change.\n\n## Trade-offs\n\n*   **Strengths:** Simple, maintainable, easy to understand, and extensible.\n*   **Weaknesses:** Might not be as feature-rich as using a dedicated frontend framework. Performance might be slightly lower than a highly optimized framework-based solution for very complex interactions (not expected here). Manual handling of DOM updates.\n"
    },
    {
      "name": "work_breakdown_structure.md",
      "type": "markdown",
      "content": "# Work Breakdown Structure\n\n```mermaid\ngraph TD\n    A[User] -->|Interacts with| B(Frontend)\n    B -->|API Requests| C(Backend Server)\n    C -->|Fetches Models| D(LLM Provider APIs: Ollama, OpenAI, ...)\n    C -->|Sends Prompts| D\n    D -->|Returns Responses| C\n    C -->|Sends Data| B\n```\n\n## Components\n\n| Component        | Responsibilities                                                                                   | Interfaces                                           | Deployment | Notes                                                            |\n| ---------------- | ---------------------------------------------------------------------------------------------------- | ---------------------------------------------------- | ---------- | ---------------------------------------------------------------- |\n| **Frontend**     | - Display UI elements (dropdowns, text editors, output area).<br>- Handle user interactions (button clicks, input).<br>- Make API calls to the backend.<br>- Render LLM output (text and code blocks). | - REST API (provided by the Backend Server)           | Local      | Runs in the user's browser.                                   |\n| **Backend Server** | - Serve static frontend files (HTML, CSS, JS).<br>- Handle API requests from the frontend.<br>- Interact with LLM provider APIs.<br>- Manage file saving.                        | - REST API (consumed by the Frontend)<br>- LLM Provider APIs | Local      | Node.js server, runs on user's machine.                        |\n| **LLM Providers** | - Provide LLM models and inference capabilities.                                                   | - Provider-specific APIs (e.g., Ollama API, OpenAI API) | External   | Accessed via HTTP requests from the Backend Server.              |\n"
    },
    {
      "name": "component_interaction.md",
      "type": "markdown",
      "content": "# Component Interaction Specifications\n\n## Frontend <-> Backend Server\n\n### 1. Get Available Providers\n\n*   **Endpoint:** `/api/providers`\n*   **Method:** `GET`\n*   **Request:** (None)\n*   **Response:** `[\"Ollama\", \"OpenAI\", ...]` (JSON array of strings)\n*   **Errors:** `500` (Internal Server Error)\n*   **Authentication:** None\n\n### 2. Get Models for Provider\n\n*   **Endpoint:** `/api/models`\n*   **Method:** `GET`\n*   **Request:** `?provider=Ollama` (Query parameter: `provider`)\n*   **Response:**  `[\"model1\", \"model2\", ...]` (JSON array of strings)\n*   **Errors:**\n    *   `400` (Bad Request - Missing or invalid provider)\n    *   `500` (Internal Server Error - Issue fetching models)\n    *   `502` (Bad Gateway - Issue communicating with the LLM provider)\n*    **Authentication:** None\n\n### 3. Send Prompt and Get Response\n\n*   **Endpoint:** `/api/generate`\n*   **Method:** `POST`\n*   **Request:** (JSON body)\n    ```json\n    {\n      \"provider\": \"Ollama\",\n      \"model\": \"llama2\",\n      \"systemPrompt\": \"You are a helpful assistant.\",\n      \"userPrompt\": \"What is the capital of France?\",\n        \"format\": \"text\" \n    }\n    ```\n*   **Response:** (JSON body)\n    ```json\n    {\n      \"output\": \"Paris is the capital of France.\",\n      \"codeBlocks\": [] \n    }\n    ```\n   or, with code blocks:\n      ```json\n      {\n        \"output\": \"Here is the python program that prints the capital of France\",\n      \"codeBlocks\": [\n          { \"name\": \"capital.py\", \"type\": \"python\", \"content\": \"print('Paris')\" }\n        ]\n      }\n    ```\n* **Errors:**\n    *    `400` (Bad request - invalid input data)\n    *    `500` (Internal server error)\n    *    `502` (Bad Gateway, if there are issues calling LLM providers)\n    *    `504` (Gateway timeout, if the LLM response takes to long - consider streaming in the future) \n* **Authentication:** None\n\n\n### 4. Save File\n\n*   **Endpoint:** `/api/save`\n*   **Method:** `POST`\n*   **Request:** (JSON body)\n    ```json\n    {\n      \"filename\": \"my_program.py\",\n      \"content\": \"print('Hello, world!')\"\n    }\n    ```\n*   **Response:**  `{ \"success\": true }` or `{ \"success\": false, \"error\": \"...\" }`\n*   **Errors:**\n    *   `400` (Bad Request - Missing filename or content)\n    *   `500` (Internal Server Error - Issue saving file)\n*   **Authentication:** None\n\n## Backend Server <-> LLM Providers\n\nThese interactions are provider-specific and follow the official API documentation for each provider (Ollama, OpenAI, etc.).  The Backend Server acts as a proxy and adapts the requests/responses to the format expected by the Frontend.\n\nKey considerations:\n\n*   **Error Handling:** The Backend Server should gracefully handle API errors from the LLM providers and return appropriate error codes to the Frontend.\n*   **Rate Limiting:** Implement rate limiting if necessary to avoid exceeding API limits.\n*   **API Keys:** Securely manage API keys for accessing the LLM providers (e.g., using environment variables).\n"
    },
    {
      "name": "implementation_tasks.md",
      "type": "markdown",
      "content": "# Implementation Tasks (for Tester)\n\n## Priority: High\n\nTASK #1: Backend Server - Get Available Providers\n\nCONTEXT:\n- Component Purpose: Provide a list of supported LLM providers to the frontend.\n- Architectural Constraints: REST API endpoint.\n- Dependencies: None.\n\nREQUIREMENTS:\n- Implement a GET endpoint `/api/providers`.\n- Return a hardcoded list of providers: `[\"Ollama\", \"OpenAI\"]`.\n\nINTERFACES:\n- GET /api/providers: Returns a JSON array of strings.\n\nACCEPTANCE CRITERIA:\n- Calling `/api/providers` returns `[\"Ollama\", \"OpenAI\"]` with HTTP status 200.\n\nPRIORITY: High\nESTIMATED COMPLEXITY: Low\n\n---\n\nTASK #2: Backend Server - Get Models for Provider\n\nCONTEXT:\n- Component Purpose:  Retrieve the available models for a given LLM provider.\n- Architectural Constraints: REST API endpoint, must handle Ollama and OpenAI.\n- Dependencies: LLM Provider APIs.\n\nREQUIREMENTS:\n- Implement a GET endpoint `/api/models?provider=<provider_name>`.\n- For `provider=Ollama`, use the Ollama API to fetch the model list.\n- For `provider=OpenAI`, use the OpenAI API to fetch the model list. (Requires OpenAI API key).\n- Return a JSON array of model names.\n\nINTERFACES:\n- GET /api/models?provider=Ollama: Returns a JSON array of Ollama model names.\n- GET /api/models?provider=OpenAI: Returns a JSON array of OpenAI model names.\n\nACCEPTANCE CRITERIA:\n- Calling `/api/models?provider=Ollama` returns a valid list of models (can be mocked initially).\n- Calling `/api/models?provider=OpenAI` returns a valid list of models (can be mocked initially).\n- Calling with an invalid provider returns a 400 error.\n- Errors from LLM provider APIs are handled gracefully (502 error).\n\nPRIORITY: High\nESTIMATED COMPLEXITY: Medium\n\n---\n\nTASK #3: Backend Server - Send Prompt and Get Response\n\nCONTEXT:\n- Component Purpose: Send the user's prompt to the selected LLM and return the response.\n- Architectural Constraints: REST API endpoint, handles both text and code block outputs.\n- Dependencies: LLM Provider APIs.\n\nREQUIREMENTS:\n- Implement a POST endpoint `/api/generate`.\n- Parse the request body (provider, model, systemPrompt, userPrompt, format).\n- Call the appropriate LLM provider API based on the `provider` and `model`.\n- Process the LLM response:\n    - If `format` is `\"text\"`, extract the plain text output.\n    - If the output contains code blocks (single or array), extract them into the `codeBlocks` array.\n- Return the response in the specified JSON format.\n\nINTERFACES:\n- POST /api/generate: Accepts a JSON body, returns a JSON response with `output` and `codeBlocks`.\n\nACCEPTANCE CRITERIA:\n- Sending a valid request to `/api/generate` returns a successful response (200) with the expected JSON format.\n- Text output is correctly extracted.\n- Code blocks are correctly identified and parsed.\n- Errors from LLM provider APIs are handled gracefully (502, 504).\n\nPRIORITY: High\nESTIMATED COMPLEXITY: High\n\n---\n\nTASK #4: Backend Server - Save File\n\nCONTEXT:\n- Component Purpose: Save a code block to a file on the server.\n- Architectural Constraints: REST API endpoint.\n- Dependencies: File system access.\n\nREQUIREMENTS:\n- Implement a POST endpoint `/api/save`.\n- Parse the request body (filename, content).\n- Save the content to a file with the given filename in a designated directory (e.g., `./saved_code/`).\n- Return a success/failure response.\n\nINTERFACES:\n- POST /api/save: Accepts a JSON body, returns a JSON response indicating success or failure.\n\nACCEPTANCE CRITERIA:\n- Sending a valid request to `/api/save` creates a file with the correct content.\n- Invalid requests (missing filename/content) return a 400 error.\n- File system errors are handled gracefully (500 error).\n\nPRIORITY: High\nESTIMATED COMPLEXITY: Medium\n\n---\n\n## Priority: Medium\n\nTASK #5: Frontend - UI Layout and Basic Interactions\n\nCONTEXT:\n- Component Purpose: Create the user interface and handle basic user input.\n- Architectural Constraints: Vanilla TypeScript, no frameworks.\n- Dependencies: None.\n\nREQUIREMENTS:\n- Create the HTML structure (dropdowns for provider and model, text editors for prompts, output area, buttons).\n- Implement event listeners for:\n    - Provider selection change.\n    - Model selection change.\n    - Load/Save buttons for system prompt.\n    - Load/Save buttons for user prompt.\n    - Submit button to send prompts.\n    - Save buttons for code blocks.\n- Populate the provider dropdown on page load.\n\nINTERFACES:\n- User interactions with UI elements.\n\nACCEPTANCE CRITERIA:\n- All UI elements are present and functional.\n- Provider dropdown is populated with \"Ollama\" and \"OpenAI\".\n- Clicking buttons triggers corresponding event handlers (can be empty functions initially).\n\nPRIORITY: Medium\nESTIMATED COMPLEXITY: Medium\n\n---\n\nTASK #6: Frontend - API Calls\n\nCONTEXT:\n- Component Purpose: Make API calls to the backend server.\n- Architectural Constraints: Use `fetch` API.\n- Dependencies: Backend Server.\n\nREQUIREMENTS:\n- Implement functions to call:\n    - `/api/providers`\n    - `/api/models?provider=<provider>`\n    - `/api/generate`\n    - `/api/save`\n- Handle responses and errors from the API calls.\n- Update the UI based on the API responses (e.g., populate the model dropdown, display LLM output).\n\nINTERFACES:\n- Calls to the backend server's REST API.\n\nACCEPTANCE CRITERIA:\n- API calls are made correctly.\n- Responses are parsed correctly.\n- Errors are handled gracefully (e.g., display error messages to the user).\n- UI is updated correctly based on API responses.\n\nPRIORITY: Medium\nESTIMATED COMPLEXITY: High\n\n---\n\n## Priority: Low\n\nTASK #7: Frontend - Code Block Rendering\n\nCONTEXT:\n- Component Purpose: Render code blocks with save buttons.\n- Architectural Constraints: Dynamically create HTML elements.\n- Dependencies: Frontend - API Calls.\n\nREQUIREMENTS:\n- When the LLM response contains `codeBlocks`, create a multiline text area for each block.\n- Set the `name` and `content` of the text area.\n- Add a 'Save' button next to each text area.\n- When the 'Save' button is clicked, call the `/api/save` endpoint with the filename and content.\n\nINTERFACES:\n- Dynamic HTML element creation and manipulation.\n\nACCEPTANCE CRITERIA:\n- Code blocks are rendered correctly.\n- Save buttons are present and functional.\n- Clicking a Save button triggers the API call to save the file.\n\nPRIORITY: Low\nESTIMATED COMPLEXITY: Medium\n"
    }
  ]
  