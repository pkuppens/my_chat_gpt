{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_LangChain Setup with Local Ollama Container Instance\n",
    "\n",
    "This notebook demonstrates how to set up LangChain to work with a local Ollama container instance.\n",
    "We will cover the installation of necessary packages, configuration, and connection to the local Ollama instance.\n",
    "This could be useful for using LangChain/LangFlow like techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Necessary Packages\n",
    "\n",
    "First, we need to install the required packages. Run the following command to install LangChain dependencies.\n",
    "\n",
    "Typically, this is already done with a `pip install -r requirements.txt`, and is only needed once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q langchain langchain_community langchain_ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Packages\n",
    "\n",
    "Next, we will import the necessary packages for our setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import TypedDict\n",
    "from langchain_ollama import OllamaLLM, ChatOllama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure Connection to Local Ollama Instance\n",
    "\n",
    "We need to configure the connection to our local Ollama container instance. The following code sets up the connection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f7dd6b",
   "metadata": {},
   "source": [
    "### Step 3.1: Setup Ollama Docker Container Instance\n",
    "\n",
    "*_Run the following step first. This step is only needed if the following step fails._*\n",
    "\n",
    "#### Ollama\n",
    "Ollama is a containerized environment for running and managing LLMs. It provides an API for interacting with the models.\n",
    "\n",
    "#### Setup Instructions\n",
    "1. Ensure Docker is installed and running on your machine.\n",
    "2. Check if there is a Docker container instance 'ollama' that can be (re-)started.\n",
    "3. _If no 'ollama' container exists_: Create and run a new Ollama container instance using the following command:\n",
    "   ```\n",
    "   docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "   ```\n",
    "4. Verify the Ollama instance is running by accessing [http://localhost:11434/api/version](http://localhost:11434/api/version).\n",
    "5. Verify programmatic connection by (re-)running the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_API_URL = \"http://localhost:11434/api\"\n",
    "\n",
    "\n",
    "def get_ollama_version():\n",
    "    response = requests.get(f\"{OLLAMA_API_URL}/version\")\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "try:\n",
    "    ollama_version = get_ollama_version()\n",
    "    if ollama_version:\n",
    "        print(f\"Connected to Ollama version: {ollama_version}\")\n",
    "    else:\n",
    "        print(\"Failed to connect to Ollama instance.\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"Failed to connect to Ollama instance, is the Docker container running?\")\n",
    "    input(\"Press Enter to continue...\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Minimal Documentation and Instructions\n",
    "\n",
    "### LangChain\n",
    "LangChain is a framework for building applications with large language models (LLMs). It provides tools and abstractions to simplify the development process.\n",
    "\n",
    "https://python.langchain.com/docs/tutorials/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Demonstrate LangChain\n",
    "\n",
    "In this step, we will demonstrate basic LangChain usage.\n",
    "\n",
    "### Example: Text Generation\n",
    "\n",
    "Use LangChain to generate text based on a prompt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd55e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OllamaLLM(model=\"llama3.2\")\n",
    "generated_text = model.invoke(\"Come up with 10 names for a song about parrots\")\n",
    "\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- [LangChain Documentation](https://python.langchain.com/docs/)\n",
    "\n",
    "### Furture Work Suggestion\n",
    "\n",
    "- [LangChain and DSpy Integration](https://www.reddit.com/r/LangChain/comments/1cqexk6/thoughts_on_dspy/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: User Input for Software Project Idea Using LangChain\n",
    "\n",
    "In this step, we will prompt the user to write a software project idea, send it to the LLM, and display the feedback, summary, and plan.\n",
    "\n",
    "### Example: User Input and Feedback Loop\n",
    "\n",
    "1. Prompt the user to write a software project idea.\n",
    "2. Send the idea to the LLM and display the feedback, summary, and plan.\n",
    "3. Implement a refinement loop to allow the user to provide additional input and receive updated feedback.\n",
    "\n",
    "#### Prompt User for Software Project Idea\n",
    "We will prompt the user to write a software project idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Get user input for software project idea, default to\n",
    "project_idea = (\n",
    "    input(\"Please write your software project idea: \")\n",
    "    or \"Create an AI software factory that generates software from project ideas, using step-by-step software processes that are implemented by LLM Agents.\"\n",
    ")\n",
    "print(project_idea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad572389",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"A user came up with a project idea: {project_idea}\n",
    "\n",
    "The user wants to know what the project would look like if it was implemented.\n",
    "\n",
    "Write a short description of the project, including the main features and how it would work.\n",
    "\n",
    "Then give your feedback on the project idea, including any suggestions for improvement, or\n",
    "areas that may need clarification.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb73e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_prompt = prompt_template.format(project_idea=project_idea)\n",
    "print(filled_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43805f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_idea_feedback = model.invoke(filled_prompt)\n",
    "print(project_idea_feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Implement Refinement Loop\n",
    "\n",
    "We will implement a refinement loop to allow the user to provide additional input and receive updated feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    additional_input = input(\n",
    "        f\"\"\"\n",
    "                             Please provide additional input to refine your project idea (or type 'exit' to finish):\n",
    "                             \n",
    "                             {project_idea}\n",
    "                             \"\"\"\n",
    "    )\n",
    "    if not additional_input.strip():\n",
    "        break\n",
    "\n",
    "    refinement_prompt_template = \"\"\"\n",
    "    You are an AI assistant. The user has provided a project description and additional input to refine the project idea.\n",
    "    Provide an updated project description that addresses the user's input.\n",
    "\n",
    "    {project_idea}\n",
    "\n",
    "    Additional Input: {additional_input}\n",
    "    \"\"\"\n",
    "\n",
    "    refinement_prompt = refinement_prompt_template.format(project_idea=project_idea, additional_input=additional_input)\n",
    "    updated_feedback_summary_plan = model.invoke(refinement_prompt)\n",
    "    print(updated_feedback_summary_plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Add TypedDict Joke Class\n",
    "\n",
    "We will add a `TypedDict` class `Joke` with fields `setup` and `punchline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Pydantic\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: str = Field(description=\"The setup of the joke, e.g. a riddle question\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke, e.g. a funny answer\")\n",
    "    rating: Optional[int] = Field(default=None, description=\"How funny the joke is, from 1 to 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f349df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "chat_ollama = ChatOllama(\n",
    "    model=\"mistral\",\n",
    "    temperature=0.7,\n",
    "    # other params...\n",
    ")\n",
    "structured_llm = chat_ollama.with_structured_output(Joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e68e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "joke = structured_llm.invoke(\"Tell me a joke about AI\")\n",
    "joke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Implement LangChain Chat Client\n",
    "\n",
    "We will implement a LangChain chat client that connects to a local Ollama container instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangChainChatClient:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model = OllamaLLM(model=model_name)\n",
    "        self.structured_llm = self.model.with_structured_output(Joke)\n",
    "\n",
    "    def generate_joke(self, prompt: str) -> Joke:\n",
    "        response = self.structured_llm.invoke(prompt)\n",
    "        print(response)\n",
    "        joke = response.split(\"\\n\")\n",
    "        return Joke(setup=joke[0], punchline=joke[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Demonstrate Chat Client Functionality\n",
    "\n",
    "We will demonstrate the chat client's functionality by generating and displaying jokes using the local Ollama model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
