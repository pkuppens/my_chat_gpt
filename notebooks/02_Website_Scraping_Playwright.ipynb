{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02_Website_Scraping_Playwright\n",
    "\n",
    "This notebook demonstrates how to scrape dynamic website content using Playwright and handle authentication for LinkedIn and Medium.\n",
    "\n",
    "We will cover the installation of necessary packages, configuration, and connection to the websites, as well as converting\n",
    "the scraped content into vector embeddings using LangChain and summarizing the content from memory in markdown format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Necessary Packages\n",
    "\n",
    "First, we need to install the required packages. Run the following command to install Playwright, LangChain, and other dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q playwright beautifulsoup4 aiohttp langchain ollama selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677e587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f880419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "import logging\n",
    "\n",
    "from utils.logger import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ef7117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinkedIn login endpoint\n",
    "login_url = \"https://www.linkedin.com/login\"\n",
    "\n",
    "# Perform login\n",
    "login_data = {\"session_key\": \"pieter.kuppens@gmail.com\", \"session_password\": input(\"Enter your LinkedIn password: \")}\n",
    "\n",
    "profile_url = \"https://www.linkedin.com/in/pieterkuppens\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0326080",
   "metadata": {},
   "source": [
    "## 1. Export Your Data First (Preferred Legal Approach)\n",
    "\n",
    "LinkedIn allows you to download your profile data directly:\n",
    "\n",
    "Go to LinkedIn Data Export. \n",
    "If no export is possible, go to: https://www.linkedin.com/mypreferences/d/settings/data-export-by-page-admins\n",
    "\n",
    "Request a full export of your data. LinkedIn will email you a ZIP file containing your profile, connections, and more.\n",
    "If this doesn't suffice, you can proceed with scraping as outlined below.\n",
    "\n",
    "Note that this notebook also intends to read and summarize linkedin articles later.\n",
    "\n",
    "### Check if your use case violates the terms of use if it exceeds personal use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bcfa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "# Set up WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.linkedin.com/login\")\n",
    "\n",
    "# Login\n",
    "username = driver.find_element(By.ID, \"username\")\n",
    "password = driver.find_element(By.ID, \"password\")\n",
    "\n",
    "username.send_keys(login_data[\"session_key\"])\n",
    "password.send_keys(login_data[\"session_password\"])\n",
    "password.send_keys(Keys.RETURN)\n",
    "\n",
    "# Wait for login to complete\n",
    "time.sleep(5)\n",
    "\n",
    "# Navigate to your profile\n",
    "driver.get(profile_url)\n",
    "\n",
    "# Get page source\n",
    "profile_page = driver.page_source\n",
    "driver.quit()\n",
    "\n",
    "# Parse the HTML with BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(profile_page, \"html.parser\")\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0b65ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
    "\n",
    "\n",
    "async def extract_soup_from_url(url):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            # async with session.post(login_url, data=login_data, headers=headers) as login_response:\n",
    "            #    login_result = login_response\n",
    "\n",
    "            async with session.get(url, headers=headers) as response:\n",
    "                response.raise_for_status()\n",
    "                html_content = await response.text()\n",
    "\n",
    "            soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        except aiohttp.ClientError as e:\n",
    "            logger.error(f\"Network error during scraping: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error in profile extraction: {e}\")\n",
    "            raise RuntimeError(\"Profile scraping failed\") from e\n",
    "\n",
    "        return soup\n",
    "\n",
    "\n",
    "soup = await extract_soup_from_url(profile_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50c1f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfortunatly, title and h1 are clear where to find, but the headline is not.\n",
    "\n",
    "{\n",
    "    \"name\": soup.find(\"h1\", class_=\"top-card-layout__title\").text.strip(),\n",
    "    \"headline\": soup.find(\"h2\").text.strip(),  # class_='top-card-layout__headline').text.strip(),\n",
    "    # \"soup\": soup\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a3133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_article_link = \"https://www.linkedin.com/pulse/why-should-you-learn-python-2022-oliver-veits\"\n",
    "\n",
    "# This probably also needs rewrite to replace the aiohttp client session with the selenium solution.\n",
    "soup_article = await extract_soup_from_url(linkedin_article_link)\n",
    "\n",
    "{\"soup\": soup_article}\n",
    "\n",
    "# Note that we detect here that a login is required to access the article content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Packages\n",
    "\n",
    "Next, we will import the necessary packages for our setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Handle Authentication\n",
    "\n",
    "We need to handle authentication for LinkedIn and Medium.\n",
    "\n",
    "The following code sets up the connection and handles the login process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login_linkedin(page, username, password):\n",
    "    # First attempt, based on Playwright, needs rewrite to aiohttp?!\n",
    "    page.goto(\"https://www.linkedin.com/login\")\n",
    "    page.fill('input[name=\"session_key\"]', username)\n",
    "    page.fill('input[name=\"session_password\"]', password)\n",
    "    page.click('button[type=\"submit\"]')\n",
    "\n",
    "\n",
    "def login_medium(page, username, password):\n",
    "    page.goto(\"https://medium.com/m/signin\")\n",
    "    page.fill('input[name=\"email\"]', username)\n",
    "    page.click('button[type=\"submit\"]')\n",
    "    # Medium sends a login link to the email, so manual intervention is needed here\n",
    "    print(\"Please check your email and click the login link sent by Medium.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcfdb69",
   "metadata": {},
   "source": [
    "## Step 4: Scrape Dynamic Website Content\n",
    "\n",
    "We will use Playwright to scrape dynamic website content from LinkedIn and Medium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b366d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_linkedin_profile(page, profile_url):\n",
    "    # Old prlaywright based code\n",
    "    page.goto(profile_url)\n",
    "    content = page.content()\n",
    "    return content\n",
    "\n",
    "\n",
    "def scrape_medium_article(page, article_url):\n",
    "    page.goto(article_url)\n",
    "    content = page.content()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36579319",
   "metadata": {},
   "source": [
    "## Step 4.1: Check if login + scraping works\n",
    "\n",
    "We'll check if the login works, by going to my personal linkedin page without login, and check what happens.\n",
    "\n",
    "If it fails, then we'll log in and try again.\n",
    "\n",
    "Then we'll repeat this for a content page that might be more hidden than a public profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2907caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_profile = \"https://www.linkedin.com/in/pieterkuppens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f54be19",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_content_no_login = scrape_linkedin_profile(page, linkedin_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Convert Scraped Content into Vector Embeddings\n",
    "\n",
    "We will use LangChain to convert the scraped content into vector embeddings and store them in a Chroma vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_embeddings(content):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_text(content)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vector_store = Chroma.from_texts(texts, embeddings)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Summarize Content from Memory\n",
    "\n",
    "We will use LangChain to summarize the content from memory in markdown format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_content(vector_store):\n",
    "    llm = OpenAI()\n",
    "    chain = LLMChain(llm=llm)\n",
    "    summary = chain.run(vector_store)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Interactive Demonstration\n",
    "\n",
    "We will demonstrate the entire process interactively, with clear markdown cells explaining each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Interactive Demonstration with Selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "def run_scraping_demo():\n",
    "    # Set up WebDriver\n",
    "    driver = webdriver.Chrome()  # Or use webdriver.Firefox() if you prefer Firefox\n",
    "\n",
    "    try:\n",
    "        # LinkedIn login and scraping\n",
    "        linkedin_username = os.getenv(\"LINKEDIN_USERNAME\", \"your_email@example.com\")\n",
    "        linkedin_password = os.getenv(\"LINKEDIN_PASSWORD\", \"your_password\")\n",
    "\n",
    "        # If environment variables aren't set, you can use the input from earlier cells\n",
    "        if not linkedin_password or linkedin_password == \"your_password\":\n",
    "            # Use the login_data dictionary if already defined in your notebook\n",
    "            try:\n",
    "                linkedin_username = login_data[\"session_key\"]\n",
    "                linkedin_password = login_data[\"session_password\"]\n",
    "            except NameError:\n",
    "                print(\"Please set your LinkedIn credentials as environment variables or in the login_data dictionary.\")\n",
    "                return\n",
    "\n",
    "        # Login to LinkedIn\n",
    "        linkedin_content = scrape_linkedin_with_selenium(driver, linkedin_username, linkedin_password)\n",
    "\n",
    "        # Convert LinkedIn content to embeddings\n",
    "        try:\n",
    "            linkedin_vector_store = convert_to_embeddings(linkedin_content)\n",
    "            linkedin_summary = summarize_content(linkedin_vector_store)\n",
    "            print(\"LinkedIn Summary:\", linkedin_summary)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing LinkedIn content: {e}\")\n",
    "\n",
    "        # Medium login and scraping\n",
    "        medium_username = os.getenv(\"MEDIUM_USERNAME\", \"your_email@example.com\")\n",
    "        medium_password = os.getenv(\"MEDIUM_PASSWORD\", \"your_password\")\n",
    "\n",
    "        # Scrape Medium article\n",
    "        try:\n",
    "            medium_article_url = \"https://medium.com/some-article\"\n",
    "            medium_content = scrape_medium_with_selenium(driver, medium_article_url, medium_username, medium_password)\n",
    "\n",
    "            # Convert Medium content to embeddings\n",
    "            medium_vector_store = convert_to_embeddings(medium_content)\n",
    "            medium_summary = summarize_content(medium_vector_store)\n",
    "            print(\"Medium Summary:\", medium_summary)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing Medium content: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Always close the driver\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "def scrape_linkedin_with_selenium(driver, username, password, profile_url=\"https://www.linkedin.com/in/pieterkuppens\"):\n",
    "    # Open LinkedIn login page\n",
    "    driver.get(\"https://www.linkedin.com/login\")\n",
    "\n",
    "    # Login\n",
    "    username_field = driver.find_element(By.ID, \"username\")\n",
    "    password_field = driver.find_element(By.ID, \"password\")\n",
    "\n",
    "    username_field.send_keys(username)\n",
    "    password_field.send_keys(password)\n",
    "    password_field.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Wait for login to complete\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Navigate to profile\n",
    "    driver.get(profile_url)\n",
    "\n",
    "    # Wait for page to load\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Get page source\n",
    "    profile_page = driver.page_source\n",
    "\n",
    "    # Parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(profile_page, \"html.parser\")\n",
    "\n",
    "    # Extract profile information\n",
    "    try:\n",
    "        name = soup.find(\"h1\", class_=\"text-heading-xlarge\").text.strip()\n",
    "        print(f\"Successfully scraped profile for: {name}\")\n",
    "    except:\n",
    "        print(\"Could not extract name from profile\")\n",
    "\n",
    "    return profile_page\n",
    "\n",
    "\n",
    "def scrape_medium_with_selenium(driver, article_url, username, password):\n",
    "    # First try to access the article without login\n",
    "    driver.get(article_url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Check if login is needed (look for login wall)\n",
    "    if \"Sign in\" in driver.page_source or \"become a member\" in driver.page_source.lower():\n",
    "        print(\"Login required for Medium. Attempting to log in...\")\n",
    "\n",
    "        # Go to Medium sign-in page\n",
    "        driver.get(\"https://medium.com/m/signin\")\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Click on \"Sign in with email\" if it exists\n",
    "        try:\n",
    "            email_button = driver.find_element(By.XPATH, \"//button[contains(text(), 'Sign in with email')]\")\n",
    "            email_button.click()\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            print(\"Could not find 'Sign in with email' button, proceeding...\")\n",
    "\n",
    "        # Enter email\n",
    "        try:\n",
    "            email_field = driver.find_element(By.NAME, \"email\")\n",
    "            email_field.send_keys(username)\n",
    "            email_field.send_keys(Keys.RETURN)\n",
    "            time.sleep(2)\n",
    "\n",
    "            print(\"Email submitted. Please check your email for a login link from Medium.\")\n",
    "            print(\"Medium requires manual login via email link. Cannot proceed automatically.\")\n",
    "            return \"Medium login requires email verification. Please login manually.\"\n",
    "        except:\n",
    "            print(\"Could not find email field for Medium login\")\n",
    "            return \"Medium login failed\"\n",
    "\n",
    "    # Get article content\n",
    "    time.sleep(3)\n",
    "    article_content = driver.page_source\n",
    "\n",
    "    # Parse with BeautifulSoup for more structured content\n",
    "    soup = BeautifulSoup(article_content, \"html.parser\")\n",
    "\n",
    "    return article_content\n",
    "\n",
    "\n",
    "# Run the scraping demo\n",
    "run_scraping_demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
