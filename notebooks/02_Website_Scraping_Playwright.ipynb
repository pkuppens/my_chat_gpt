{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02_Website_Scraping_Playwright\n",
    "\n",
    "This notebook demonstrates how to scrape dynamic website content using Playwright and handle authentication for LinkedIn and Medium.\n",
    "\n",
    "We will cover the installation of necessary packages, configuration, and connection to the websites, as well as converting\n",
    "the scraped content into vector embeddings using LangChain and summarizing the content from memory in markdown format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Necessary Packages\n",
    "\n",
    "First, we need to install the required packages. Run the following command to install Playwright, LangChain, and other dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q playwright browser-use beautifulsoup4 aiohttp langchain ollama selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677e587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f880419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s: %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40ef7117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinkedIn login endpoint\n",
    "login_url = 'https://www.linkedin.com/login'\n",
    "\n",
    "# Perform login\n",
    "login_data = {\n",
    "    'session_key': 'pieter.kuppens@gmail.com',\n",
    "    'session_password': input('Enter your LinkedIn password: ')\n",
    "}\n",
    "\n",
    "profile_url = \"https://www.linkedin.com/in/pieterkuppens\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0326080",
   "metadata": {},
   "source": [
    "## 1. Export Your Data First (Preferred Legal Approach)\n",
    "\n",
    "LinkedIn allows you to download your profile data directly:\n",
    "\n",
    "Go to LinkedIn Data Export. \n",
    "If no export is possible, go to: https://www.linkedin.com/mypreferences/d/settings/data-export-by-page-admins\n",
    "\n",
    "Request a full export of your data. LinkedIn will email you a ZIP file containing your profile, connections, and more.\n",
    "If this doesn't suffice, you can proceed with scraping as outlined below.\n",
    "\n",
    "Note that this notebook also intends to read and summarize linkedin articles later.\n",
    "\n",
    "### Check if your use case violates the terms of use if it exceeds personal use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bcfa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "# Set up WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.linkedin.com/login\")\n",
    "\n",
    "# Login\n",
    "username = driver.find_element(By.ID, \"username\")\n",
    "password = driver.find_element(By.ID, \"password\")\n",
    "\n",
    "username.send_keys(login_data['session_key'])\n",
    "password.send_keys(login_data['session_password'])\n",
    "password.send_keys(Keys.RETURN)\n",
    "\n",
    "# Wait for login to complete\n",
    "time.sleep(5)\n",
    "\n",
    "# Navigate to your profile\n",
    "driver.get(profile_url)\n",
    "\n",
    "# Get page source\n",
    "profile_page = driver.page_source\n",
    "driver.quit()\n",
    "\n",
    "# Parse the HTML with BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(profile_page, 'html.parser')\n",
    "print(soup.prettify())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b0b65ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "}\n",
    "\n",
    "async def extract_soup_from_url(url):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            # async with session.post(login_url, data=login_data, headers=headers) as login_response:\n",
    "            #    login_result = login_response\n",
    "\n",
    "            async with session.get(url, headers=headers) as response:\n",
    "                response.raise_for_status()\n",
    "                html_content = await response.text()\n",
    "            \n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        except aiohttp.ClientError as e:\n",
    "            logger.error(f\"Network error during scraping: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error in profile extraction: {e}\")\n",
    "            raise RuntimeError(\"Profile scraping failed\") from e\n",
    "        \n",
    "        return soup\n",
    "\n",
    "soup = await extract_soup_from_url(profile_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50c1f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfortunatly, title and h1 are clear where to find, but the headline is not.\n",
    "\n",
    "{\n",
    "    \"name\": soup.find('h1', class_='top-card-layout__title').text.strip(),\n",
    "    \"headline\": soup.find('h2').text.strip(),  # class_='top-card-layout__headline').text.strip(),\n",
    "    # \"soup\": soup\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a3133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_article_link = \"https://www.linkedin.com/pulse/why-should-you-learn-python-2022-oliver-veits\"\n",
    "\n",
    "# This probably also needs rewrite to replace the aiohttp client session with the selenium solution.\n",
    "soup_article = await extract_soup_from_url(linkedin_article_link)\n",
    "\n",
    "{\n",
    "    \"soup\": soup_article\n",
    "}\n",
    "\n",
    "# Note that we detect here that a login is required to access the article content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Packages\n",
    "\n",
    "Next, we will import the necessary packages for our setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Handle Authentication\n",
    "\n",
    "We need to handle authentication for LinkedIn and Medium.\n",
    "\n",
    "The following code sets up the connection and handles the login process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login_linkedin(page, username, password):\n",
    "    # First attempt, based on Playwright, needs rewrite to aiohttp?!\n",
    "    page.goto('https://www.linkedin.com/login')\n",
    "    page.fill('input[name=\"session_key\"]', username)\n",
    "    page.fill('input[name=\"session_password\"]', password)\n",
    "    page.click('button[type=\"submit\"]')\n",
    "\n",
    "def login_medium(page, username, password):\n",
    "    page.goto('https://medium.com/m/signin')\n",
    "    page.fill('input[name=\"email\"]', username)\n",
    "    page.click('button[type=\"submit\"]')\n",
    "    # Medium sends a login link to the email, so manual intervention is needed here\n",
    "    print(\"Please check your email and click the login link sent by Medium.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcfdb69",
   "metadata": {},
   "source": [
    "## Step 4: Scrape Dynamic Website Content\n",
    "\n",
    "We will use Playwright to scrape dynamic website content from LinkedIn and Medium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b366d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_linkedin_profile(page, profile_url):\n",
    "    # Old prlaywright based code\n",
    "    page.goto(profile_url)\n",
    "    content = page.content()\n",
    "    return content\n",
    "\n",
    "def scrape_medium_article(page, article_url):\n",
    "    page.goto(article_url)\n",
    "    content = page.content()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36579319",
   "metadata": {},
   "source": [
    "## Step 4.1: Check if login + scraping works\n",
    "\n",
    "We'll check if the login works, by going to my personal linkedin page without login, and check what happens.\n",
    "\n",
    "If it fails, then we'll log in and try again.\n",
    "\n",
    "Then we'll repeat this for a content page that might be more hidden than a public profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2907caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_profile = \"https://www.linkedin.com/in/pieterkuppens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f54be19",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_content_no_login = scrape_linkedin_profile(page, linkedin_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Convert Scraped Content into Vector Embeddings\n",
    "\n",
    "We will use LangChain to convert the scraped content into vector embeddings and store them in a Chroma vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_embeddings(content):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_text(content)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vector_store = Chroma.from_texts(texts, embeddings)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Summarize Content from Memory\n",
    "\n",
    "We will use LangChain to summarize the content from memory in markdown format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_content(vector_store):\n",
    "    llm = OpenAI()\n",
    "    chain = LLMChain(llm=llm)\n",
    "    summary = chain.run(vector_store)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Interactive Demonstration\n",
    "\n",
    "We will demonstrate the entire process interactively, with clear markdown cells explaining each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sync_playwright() as p:\n",
    "    browser = p.chromium.launch(headless=False)\n",
    "    page = browser.new_page()\n",
    "\n",
    "    # LinkedIn login\n",
    "    linkedin_username = os.getenv('LINKEDIN_USERNAME')\n",
    "    linkedin_password = os.getenv('LINKEDIN_PASSWORD')\n",
    "    login_linkedin(page, linkedin_username, linkedin_password)\n",
    "\n",
    "    # Scrape LinkedIn profile\n",
    "    linkedin_profile_url = 'https://www.linkedin.com/in/some-profile/'\n",
    "    linkedin_content = scrape_linkedin_profile(page, linkedin_profile_url)\n",
    "\n",
    "    # Convert LinkedIn content to embeddings\n",
    "    linkedin_vector_store = convert_to_embeddings(linkedin_content)\n",
    "\n",
    "    # Summarize LinkedIn content\n",
    "    linkedin_summary = summarize_content(linkedin_vector_store)\n",
    "    print(\"LinkedIn Summary:\", linkedin_summary)\n",
    "\n",
    "    # Medium login\n",
    "    medium_username = os.getenv('MEDIUM_USERNAME')\n",
    "    medium_password = os.getenv('MEDIUM_PASSWORD')\n",
    "    login_medium(page, medium_username, medium_password)\n",
    "\n",
    "    # Scrape Medium article\n",
    "    medium_article_url = 'https://medium.com/some-article'\n",
    "    medium_content = scrape_medium_article(page, medium_article_url)\n",
    "\n",
    "    # Convert Medium content to embeddings\n",
    "    medium_vector_store = convert_to_embeddings(medium_content)\n",
    "\n",
    "    # Summarize Medium content\n",
    "    medium_summary = summarize_content(medium_vector_store)\n",
    "    print(\"Medium Summary:\", medium_summary)\n",
    "\n",
    "    browser.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-chat-gpt-kernel",
   "language": "python",
   "name": "my-chat-gpt-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
