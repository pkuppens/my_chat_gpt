{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02_Website_Scraping_Playwright\n",
    "\n",
    "This notebook demonstrates how to scrape dynamic website content using Playwright and handle authentication for LinkedIn and Medium.\n",
    "\n",
    "We will cover the installation of necessary packages, configuration, and connection to the websites, as well as converting\n",
    "the scraped content into vector embeddings using LangChain and summarizing the content from memory in markdown format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Necessary Packages\n",
    "\n",
    "First, we need to install the required packages. Run the following command to install Playwright, LangChain, and other dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q playwright browser-use beautifulsoup4 aiohttp langchain ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f880419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s: %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b0b65ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "profile_url = \"https://www.linkedin.com/in/pieterkuppens\"\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "}\n",
    "\n",
    "async def extract_soup_from_url(url):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.get(url, headers=headers) as response:\n",
    "                response.raise_for_status()\n",
    "                html_content = await response.text()\n",
    "            \n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        except aiohttp.ClientError as e:\n",
    "            logger.error(f\"Network error during scraping: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error in profile extraction: {e}\")\n",
    "            raise RuntimeError(\"Profile scraping failed\") from e\n",
    "        \n",
    "        return soup\n",
    "\n",
    "soup = await extract_soup_from_url(profile_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50c1f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"name\": soup.find('h1', class_='top-card-layout__title').text.strip(),\n",
    "    \"headline\": soup.find('h2').text.strip(),  # class_='top-card-layout__headline').text.strip(),\n",
    "    # \"soup\": soup\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a3133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_article_link = \"https://www.linkedin.com/pulse/why-should-you-learn-python-2022-oliver-veits\"\n",
    "\n",
    "soup_article = await extract_soup_from_url(linkedin_article_link)\n",
    "\n",
    "{\n",
    "    \"soup\": soup_article\n",
    "}\n",
    "\n",
    "# Note that we detect here that a login is required to access the article content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Packages\n",
    "\n",
    "Next, we will import the necessary packages for our setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Handle Authentication\n",
    "\n",
    "We need to handle authentication for LinkedIn and Medium.\n",
    "\n",
    "The following code sets up the connection and handles the login process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login_linkedin(page, username, password):\n",
    "    # First attempt, based on Playwright, needs rewrite to aiohttp?!\n",
    "    page.goto('https://www.linkedin.com/login')\n",
    "    page.fill('input[name=\"session_key\"]', username)\n",
    "    page.fill('input[name=\"session_password\"]', password)\n",
    "    page.click('button[type=\"submit\"]')\n",
    "\n",
    "def login_medium(page, username, password):\n",
    "    page.goto('https://medium.com/m/signin')\n",
    "    page.fill('input[name=\"email\"]', username)\n",
    "    page.click('button[type=\"submit\"]')\n",
    "    # Medium sends a login link to the email, so manual intervention is needed here\n",
    "    print(\"Please check your email and click the login link sent by Medium.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcfdb69",
   "metadata": {},
   "source": [
    "## Step 4: Scrape Dynamic Website Content\n",
    "\n",
    "We will use Playwright to scrape dynamic website content from LinkedIn and Medium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b366d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_linkedin_profile(page, profile_url):\n",
    "    # Old prlaywright based code\n",
    "    page.goto(profile_url)\n",
    "    content = page.content()\n",
    "    return content\n",
    "\n",
    "def scrape_medium_article(page, article_url):\n",
    "    page.goto(article_url)\n",
    "    content = page.content()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36579319",
   "metadata": {},
   "source": [
    "## Step 4.1: Check if login + scraping works\n",
    "\n",
    "We'll check if the login works, by going to my personal linkedin page without login, and check what happens.\n",
    "\n",
    "If it fails, then we'll log in and try again.\n",
    "\n",
    "Then we'll repeat this for a content page that might be more hidden than a public profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2907caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_profile = \"https://www.linkedin.com/in/pieterkuppens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f54be19",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_content_no_login = scrape_linkedin_profile(page, linkedin_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Convert Scraped Content into Vector Embeddings\n",
    "\n",
    "We will use LangChain to convert the scraped content into vector embeddings and store them in a Chroma vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_embeddings(content):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_text(content)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vector_store = Chroma.from_texts(texts, embeddings)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Summarize Content from Memory\n",
    "\n",
    "We will use LangChain to summarize the content from memory in markdown format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_content(vector_store):\n",
    "    llm = OpenAI()\n",
    "    chain = LLMChain(llm=llm)\n",
    "    summary = chain.run(vector_store)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Interactive Demonstration\n",
    "\n",
    "We will demonstrate the entire process interactively, with clear markdown cells explaining each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sync_playwright() as p:\n",
    "    browser = p.chromium.launch(headless=False)\n",
    "    page = browser.new_page()\n",
    "\n",
    "    # LinkedIn login\n",
    "    linkedin_username = os.getenv('LINKEDIN_USERNAME')\n",
    "    linkedin_password = os.getenv('LINKEDIN_PASSWORD')\n",
    "    login_linkedin(page, linkedin_username, linkedin_password)\n",
    "\n",
    "    # Scrape LinkedIn profile\n",
    "    linkedin_profile_url = 'https://www.linkedin.com/in/some-profile/'\n",
    "    linkedin_content = scrape_linkedin_profile(page, linkedin_profile_url)\n",
    "\n",
    "    # Convert LinkedIn content to embeddings\n",
    "    linkedin_vector_store = convert_to_embeddings(linkedin_content)\n",
    "\n",
    "    # Summarize LinkedIn content\n",
    "    linkedin_summary = summarize_content(linkedin_vector_store)\n",
    "    print(\"LinkedIn Summary:\", linkedin_summary)\n",
    "\n",
    "    # Medium login\n",
    "    medium_username = os.getenv('MEDIUM_USERNAME')\n",
    "    medium_password = os.getenv('MEDIUM_PASSWORD')\n",
    "    login_medium(page, medium_username, medium_password)\n",
    "\n",
    "    # Scrape Medium article\n",
    "    medium_article_url = 'https://medium.com/some-article'\n",
    "    medium_content = scrape_medium_article(page, medium_article_url)\n",
    "\n",
    "    # Convert Medium content to embeddings\n",
    "    medium_vector_store = convert_to_embeddings(medium_content)\n",
    "\n",
    "    # Summarize Medium content\n",
    "    medium_summary = summarize_content(medium_vector_store)\n",
    "    print(\"Medium Summary:\", medium_summary)\n",
    "\n",
    "    browser.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-chat-gpt-kernel",
   "language": "python",
   "name": "my-chat-gpt-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
