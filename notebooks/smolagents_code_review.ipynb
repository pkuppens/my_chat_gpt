{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Code Review Agent\n",
    "\n",
    "This notebook demonstrates the use of the `smolagents` library to create an agent that:\n",
    "1. Reads a Python file.\n",
    "2. Parses it to extract function signatures and class definitions.\n",
    "3. Reviews these functions using a Large Language Model (LLM).\n",
    "4. Documents the findings in a Markdown file.\n",
    "\n",
    "Ensure you have the `smolagents` library installed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q smolagents litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "from smolagents import CodeAgent, HfApiModel, LiteLLMModel, tool\n",
    "\n",
    "model_id = (\"ollama_chat/llama3.2\",)\n",
    "\n",
    "model_id = \"ollama/deepseek-coder-v2:latest\"\n",
    "model_id = \"ollama/codellama:13b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LiteLLMModel(model_id=model_id, api_key=\"ollama\")\n",
    "agent = CodeAgent(tools=[], model=model, add_base_tools=True)\n",
    "\n",
    "agent.run(\"Could you give me the 7th Fibonacci number, integer, from recursive calculation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def read_python_file(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns the content of a Python file.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the Python file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def parse_python_code(code: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns the parsed functions and class definitions from Python code.\n",
    "\n",
    "    Args:\n",
    "        code: Python code as a string.\n",
    "\n",
    "    Returns:\n",
    "        list: List of function and class signatures.\n",
    "    \"\"\"\n",
    "    tree = ast.parse(code)\n",
    "    definitions = []\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.FunctionDef):\n",
    "            definitions.append(f\"Function: {node.name}{ast.unparse(node.args)}\")\n",
    "        elif isinstance(node, ast.ClassDef):\n",
    "            definitions.append(f\"Class: {node.name}\")\n",
    "    return definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def write_findings_to_markdown(findings: list[str], output_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Writes the review findings to a Markdown file.\n",
    "\n",
    "    Args:\n",
    "        findings: List of tuples containing code signatures and their reviews.\n",
    "        output_path: Path to the output Markdown file.\n",
    "    \"\"\"\n",
    "    result = \"# Code Review Findings\\n\\n\"\n",
    "\n",
    "    for signature, review in findings:\n",
    "        result += f\"## {signature}\\n\"\n",
    "        result += f\"**Review Findings:** {review}\\n\\n\"\n",
    "\n",
    "    with open(output_path, \"w\") as md_file:\n",
    "        md_file.write(result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_code_with_llm(agent, code_snippet):\n",
    "    \"\"\"\n",
    "    Uses the LLM agent to review a code snippet.\n",
    "\n",
    "    Args:\n",
    "        agent (CodeAgent): The initialized CodeAgent.\n",
    "        code_snippet (str): The code snippet to review.\n",
    "\n",
    "    Returns:\n",
    "        str: Review findings from the LLM.\n",
    "    \"\"\"\n",
    "    prompt = f\"Review the following Python code:\\n\\n{code_snippet}\\n\\nProvide feedback:\"\n",
    "    return agent.run(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "# Where am I?\n",
    "# os.getcwd()\n",
    "print(pathlib.Path().absolute())\n",
    "\n",
    "# Note: this is the project root directory!\n",
    "file_to_review = pathlib.Path(\"notebooks/review_test_script.py\")\n",
    "\n",
    "assert file_to_review.exists(), \"File not found!\"\n",
    "\n",
    "print(str(file_to_review))\n",
    "\n",
    "results_file_name = file_to_review.with_suffix(\".md\")\n",
    "print(str(results_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have your Hugging Face API token set in the environment\n",
    "api_key = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "if api_key:\n",
    "    print(\"API key found, using Hugging Face API Model.\")\n",
    "    model = HfApiModel(model_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\", api_key=api_key)\n",
    "else:\n",
    "    print(\"API key not found, using Local Ollama Model.\")\n",
    "    model_id = \"codellama:13b\"\n",
    "    model = LiteLLMModel(model_id=model_id, api_key=\"ollama\")\n",
    "\n",
    "agent = CodeAgent(model=model, tools=[parse_python_code, read_python_file, write_findings_to_markdown])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = f\"\"\"\n",
    "Review the code in the file `{file_to_review}` and provide feedback on the functions and classes defined in the file.\n",
    "\n",
    "Write the review findings to a Markdown file: `{results_file_name}`.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.run(task)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Main Execution\n",
    "# Specify the path to the Python file to be reviewed\n",
    "\n",
    "python_file_path = \"path/to/your_script.py\"\n",
    "# Specify the path for the output Markdown file\n",
    "output_markdown_path = \"code_review_findings.md\"\n",
    "\n",
    "# Step 1: Read the Python file\n",
    "code_content = read_python_file(python_file_path)\n",
    "\n",
    "# Step 2: Parse the code to extract definitions\n",
    "definitions = parse_python_code(code_content)\n",
    "\n",
    "# Step 3: Review each definition using the LLM\n",
    "findings = []\n",
    "for definition in definitions:\n",
    "    review = review_code_with_llm(agent, definition)\n",
    "    findings.append((definition, review))\n",
    "\n",
    "# Step 4: Write the findings to a Markdown file\n",
    "write_findings_to_markdown(findings, output_markdown_path)\n",
    "\n",
    "print(f\"Code review completed. Findings are saved in {output_markdown_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof of Concept: Example Review Results on a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def review_file(file_path: str, style_guide: str) -> str:\n",
    "    \"\"\"Review a file for a certain style guide.\n",
    "\n",
    "    Args:\n",
    "        file_path: The path to the file to review.\n",
    "        style_guide: The style guide to review against.\n",
    "\n",
    "    Returns:\n",
    "        str: The review results.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Placeholder for actual review logic\n",
    "    review_results = f\"Reviewing '{file_path}' against {style_guide}...\\n\"\n",
    "\n",
    "    # TODO: run the actual review logic, passing content and style_guide to an LLM\n",
    "    review_results += \"\\nNot yet implemented.\"\n",
    "\n",
    "    return review_results\n",
    "\n",
    "\n",
    "example_file_path = \"notebooks/review_test_script.py\"\n",
    "example_style_guide = \"PEP 8\"\n",
    "\n",
    "review_results = review_file(example_file_path, example_style_guide)\n",
    "print(review_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
