{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OpenAI Agent SDK with Guardrails - Proof of Concept\n",
        "\n",
        "This notebook demonstrates a well-designed, production-ready implementation of agents with swappable LLM providers and guardrails.\n",
        "\n",
        "## Architecture Overview\n",
        "\n",
        "The implementation follows SOLID principles with swappable components:\n",
        "\n",
        "1. **LLM Providers**: Abstract interface allowing OpenAI, Ollama, or other providers\n",
        "2. **Guardrail Providers**: Pluggable guardrails using OpenAI, Gemini, or local models\n",
        "3. **Agent System**: Agents loaded from SuperPrompt templates with tool support\n",
        "4. **Tool System**: File operations and inter-agent communication\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- \u2705 Swappable LLM providers (OpenAI, Ollama)\n",
        "- \u2705 Swappable guardrail providers (OpenAI, Gemini, Local)\n",
        "- \u2705 Agent creation from SuperPrompt files\n",
        "- \u2705 Agent-to-agent communication and delegation\n",
        "- \u2705 File-saving tools\n",
        "- \u2705 Prompt composition and reuse\n",
        "- \u2705 Input/Output guardrails\n",
        "\n",
        "## Implementation\n",
        "\n",
        "The core implementation is in `my_chat_gpt_utils/agents_sdk.py` for better modularity and reusability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Add parent directory to path\n",
        "sys.path.insert(0, os.path.abspath('..'))\n",
        "\n",
        "# Import our agents SDK\n",
        "from my_chat_gpt_utils.agents_sdk import (\n",
        "    # Providers\n",
        "    OpenAIProvider,\n",
        "    OllamaProvider,\n",
        "    OpenAIGuardrailProvider,\n",
        "    GeminiGuardrailProvider,\n",
        "    LocalGuardrailProvider,\n",
        "    # Core components\n",
        "    Agent,\n",
        "    AgentConfig,\n",
        "    AgentOrchestrator,\n",
        "    SuperPromptLoader,\n",
        "    ToolRegistry,\n",
        "    # Data models\n",
        "    Message,\n",
        "    GuardrailResult,\n",
        ")\n",
        "\n",
        "print(\"\u2713 Imports successful\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initialize LLM Providers\n",
        "\n",
        "The design allows you to easily swap between different LLM providers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: OpenAI Provider\n",
        "try:\n",
        "    openai_provider = OpenAIProvider(model=\"gpt-4o-mini\")\n",
        "    print(\"\u2713 OpenAI provider initialized\")\n",
        "except Exception as e:\n",
        "    print(f\"\u26a0 OpenAI provider failed: {e}\")\n",
        "    openai_provider = None\n",
        "\n",
        "# Option 2: Ollama Provider (requires Ollama running locally)\n",
        "try:\n",
        "    ollama_provider = OllamaProvider(model=\"llama3.2\")\n",
        "    print(\"\u2713 Ollama provider initialized\")\n",
        "except Exception as e:\n",
        "    print(f\"\u26a0 Ollama provider failed: {e}\")\n",
        "    ollama_provider = None\n",
        "\n",
        "# Select which provider to use\n",
        "llm_provider = openai_provider if openai_provider else ollama_provider\n",
        "\n",
        "if llm_provider:\n",
        "    print(f\"\\n\u2713 Using LLM provider: {llm_provider.__class__.__name__}\")\n",
        "else:\n",
        "    print(\"\\n\u26a0 No LLM provider available. Please configure OpenAI or Ollama.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize Guardrail Providers\n",
        "\n",
        "Guardrails can be swapped to use different providers (e.g., use Gemini for guardrails on OpenAI prompts)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: OpenAI Guardrails (moderation API)\n",
        "try:\n",
        "    openai_guardrail = OpenAIGuardrailProvider()\n",
        "    print(\"\u2713 OpenAI guardrail provider initialized\")\n",
        "except Exception as e:\n",
        "    print(f\"\u26a0 OpenAI guardrail failed: {e}\")\n",
        "    openai_guardrail = None\n",
        "\n",
        "# Option 2: Gemini Guardrails\n",
        "try:\n",
        "    gemini_guardrail = GeminiGuardrailProvider()\n",
        "    print(\"\u2713 Gemini guardrail provider initialized\")\n",
        "except Exception as e:\n",
        "    print(f\"\u26a0 Gemini guardrail failed: {e}\")\n",
        "    gemini_guardrail = None\n",
        "\n",
        "# Option 3: Local Guardrails (always available)\n",
        "local_guardrail = LocalGuardrailProvider()\n",
        "print(\"\u2713 Local guardrail provider initialized\")\n",
        "\n",
        "# Select guardrails (you can mix and match)\n",
        "input_guardrail = openai_guardrail if openai_guardrail else local_guardrail\n",
        "output_guardrail = local_guardrail  # Use local for output by default\n",
        "\n",
        "print(f\"\\n\u2713 Using input guardrail: {input_guardrail.__class__.__name__}\")\n",
        "print(f\"\u2713 Using output guardrail: {output_guardrail.__class__.__name__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Initialize Core Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize components\n",
        "prompt_loader = SuperPromptLoader()\n",
        "tool_registry = ToolRegistry()\n",
        "orchestrator = AgentOrchestrator()\n",
        "\n",
        "print(\"\u2713 Core components initialized\\n\")\n",
        "print(\"Available SuperPrompts:\")\n",
        "prompts = prompt_loader.list_prompts()\n",
        "for i, prompt in enumerate(prompts[:10], 1):  # Show first 10\n",
        "    print(f\"  {i}. {prompt}\")\n",
        "if len(prompts) > 10:\n",
        "    print(f\"  ... and {len(prompts) - 10} more\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Example 1: Create Agent from SuperPrompt\n",
        "\n",
        "Agents are created from SuperPrompt templates, with tools and guardrails."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an architect agent from SuperPrompt\n",
        "architect_config = AgentConfig(\n",
        "    name=\"Software Architect\",\n",
        "    system_prompt=prompt_loader.load_prompt(\"software_architect_ai_prompt.txt\"),\n",
        "    temperature=0.7,\n",
        "    tools=tool_registry.get_tool_definitions()\n",
        ")\n",
        "\n",
        "architect_agent = orchestrator.create_agent(\n",
        "    name=\"architect\",\n",
        "    config=architect_config,\n",
        "    llm_provider=llm_provider,\n",
        "    tool_registry=tool_registry,\n",
        "    input_guardrail=input_guardrail,\n",
        "    output_guardrail=output_guardrail\n",
        ")\n",
        "\n",
        "print(\"\u2713 Architect agent created with:\")\n",
        "print(f\"  - LLM: {llm_provider.__class__.__name__ if llm_provider else 'None'}\")\n",
        "print(f\"  - Input Guardrail: {input_guardrail.__class__.__name__}\")\n",
        "print(f\"  - Output Guardrail: {output_guardrail.__class__.__name__}\")\n",
        "print(f\"  - Tools: {len(tool_registry.get_tool_definitions())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the architect agent\n",
        "if llm_provider:\n",
        "    response = architect_agent.run(\n",
        "        \"Design a high-level architecture for a simple REST API with user authentication. \"\n",
        "        \"Save the architecture to a file called 'api_architecture.md'.\"\n",
        "    )\n",
        "    print(\"\\nArchitect Response:\")\n",
        "    print(response)\n",
        "else:\n",
        "    print(\"\u26a0 No LLM provider available. Please configure OpenAI or Ollama.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Example 2: Multi-Agent Collaboration\n",
        "\n",
        "Multiple agents can work together, each with their own specialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a coder agent\n",
        "coder_config = AgentConfig(\n",
        "    name=\"Python Programmer\",\n",
        "    system_prompt=prompt_loader.load_prompt(\"python-programmer-superprompt.md\"),\n",
        "    temperature=0.3,  # Lower temperature for code generation\n",
        "    tools=tool_registry.get_tool_definitions()\n",
        ")\n",
        "\n",
        "coder_agent = orchestrator.create_agent(\n",
        "    name=\"coder\",\n",
        "    config=coder_config,\n",
        "    llm_provider=llm_provider,\n",
        "    tool_registry=tool_registry,\n",
        "    input_guardrail=input_guardrail,\n",
        "    output_guardrail=output_guardrail\n",
        ")\n",
        "\n",
        "print(\"\u2713 Coder agent created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test coder agent\n",
        "if llm_provider:\n",
        "    response = coder_agent.run(\n",
        "        \"Write a Python function that validates email addresses using regex. \"\n",
        "        \"Include docstrings and save it to 'email_validator.py'.\"\n",
        "    )\n",
        "    print(\"\\nCoder Response:\")\n",
        "    print(response)\n",
        "else:\n",
        "    print(\"\u26a0 No LLM provider available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Example 3: Meta-Agent Creating Other Agents\n",
        "\n",
        "A meta-agent can design and create other specialist agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a meta-agent that can create other agents\n",
        "meta_agent_prompt = \"\"\"You are a Meta-Agent that specializes in creating and coordinating other AI agents.\n",
        "\n",
        "Your capabilities:\n",
        "1. Analyze tasks and determine what specialist agents are needed\n",
        "2. Create agent configurations with appropriate prompts and tools\n",
        "3. Coordinate work between multiple agents\n",
        "4. Synthesize results from multiple agents\n",
        "\n",
        "When asked to create an agent, provide:\n",
        "- Agent name\n",
        "- Agent role and responsibilities\n",
        "- Specific system prompt for the agent\n",
        "- What tools the agent should have access to\"\"\"\n",
        "\n",
        "meta_config = AgentConfig(\n",
        "    name=\"Meta-Agent\",\n",
        "    system_prompt=meta_agent_prompt,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "meta_agent = orchestrator.create_agent(\n",
        "    name=\"meta\",\n",
        "    config=meta_config,\n",
        "    llm_provider=llm_provider,\n",
        "    input_guardrail=input_guardrail,\n",
        "    output_guardrail=output_guardrail\n",
        ")\n",
        "\n",
        "print(\"\u2713 Meta-agent created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ask meta-agent to design a new specialist agent\n",
        "if llm_provider:\n",
        "    response = meta_agent.run(\n",
        "        \"I need to build a system for automated code review. \"\n",
        "        \"Design a specialist agent that can review Python code for quality, \"\n",
        "        \"security issues, and best practices. Provide the agent configuration.\"\n",
        "    )\n",
        "    print(\"\\nMeta-Agent Response:\")\n",
        "    print(response)\n",
        "else:\n",
        "    print(\"\u26a0 No LLM provider available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Example 4: Prompt Composition and Reuse\n",
        "\n",
        "Multiple SuperPrompts can be composed together to create more capable agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compose multiple SuperPrompts\n",
        "composed_prompt = prompt_loader.compose_prompts(\n",
        "    \"3tier.md\",\n",
        "    \"agentic_ai_framework_coder.md\"\n",
        ")\n",
        "\n",
        "# Create agent with composed prompt\n",
        "fullstack_config = AgentConfig(\n",
        "    name=\"Full-Stack AI Developer\",\n",
        "    system_prompt=composed_prompt,\n",
        "    temperature=0.5,\n",
        "    tools=tool_registry.get_tool_definitions()\n",
        ")\n",
        "\n",
        "fullstack_agent = orchestrator.create_agent(\n",
        "    name=\"fullstack\",\n",
        "    config=fullstack_config,\n",
        "    llm_provider=llm_provider,\n",
        "    tool_registry=tool_registry,\n",
        "    input_guardrail=input_guardrail,\n",
        "    output_guardrail=output_guardrail\n",
        ")\n",
        "\n",
        "print(\"\u2713 Full-stack agent created with composed prompts\")\n",
        "print(f\"  Prompt length: {len(composed_prompt)} characters\")\n",
        "print(f\"  Composed from: 3tier.md + agentic_ai_framework_coder.md\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Example 5: Testing Guardrails\n",
        "\n",
        "Guardrails protect against inappropriate content and sensitive data leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test different guardrail providers\n",
        "test_inputs = [\n",
        "    \"Write a simple hello world program\",  # Safe\n",
        "    \"My API key is sk-1234567890abcdef\" * 100,  # Contains sensitive data + too long\n",
        "    \"Create a secure authentication system\",  # Safe\n",
        "]\n",
        "\n",
        "print(\"Testing Guardrails:\\n\")\n",
        "\n",
        "for i, test_input in enumerate(test_inputs, 1):\n",
        "    print(f\"Test {i}: {test_input[:60]}{'...' if len(test_input) > 60 else ''}\")\n",
        "    \n",
        "    # Test local guardrail\n",
        "    result = local_guardrail.check(test_input)\n",
        "    print(f\"  Local: {'\u2713 PASS' if result.passed else '\u2717 FAIL'} - {result.message}\")\n",
        "    \n",
        "    # Test OpenAI guardrail if available\n",
        "    if openai_guardrail:\n",
        "        result = openai_guardrail.check(test_input)\n",
        "        print(f\"  OpenAI: {'\u2713 PASS' if result.passed else '\u2717 FAIL'} - {result.message}\")\n",
        "    \n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Example 6: Demonstrating Component Swapping\n",
        "\n",
        "The architecture allows easy swapping of LLM and guardrail providers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate component swapping\n",
        "print(\"Component Swapping Demo\\n\")\n",
        "\n",
        "# Create the same agent with different configurations\n",
        "simple_prompt = \"You are a helpful AI assistant that explains technical concepts clearly.\"\n",
        "simple_config = AgentConfig(\n",
        "    name=\"Simple Assistant\",\n",
        "    system_prompt=simple_prompt,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "configs = []\n",
        "\n",
        "# Configuration 1: OpenAI + OpenAI Guardrails\n",
        "if openai_provider and openai_guardrail:\n",
        "    agent_v1 = Agent(\n",
        "        config=simple_config,\n",
        "        llm_provider=openai_provider,\n",
        "        input_guardrail=openai_guardrail,\n",
        "        output_guardrail=openai_guardrail\n",
        "    )\n",
        "    configs.append(\"Config 1: OpenAI LLM + OpenAI Guardrails\")\n",
        "\n",
        "# Configuration 2: OpenAI + Local Guardrails\n",
        "if openai_provider:\n",
        "    agent_v2 = Agent(\n",
        "        config=simple_config,\n",
        "        llm_provider=openai_provider,\n",
        "        input_guardrail=local_guardrail,\n",
        "        output_guardrail=local_guardrail\n",
        "    )\n",
        "    configs.append(\"Config 2: OpenAI LLM + Local Guardrails\")\n",
        "\n",
        "# Configuration 3: Ollama + Local Guardrails\n",
        "if ollama_provider:\n",
        "    agent_v3 = Agent(\n",
        "        config=simple_config,\n",
        "        llm_provider=ollama_provider,\n",
        "        input_guardrail=local_guardrail,\n",
        "        output_guardrail=local_guardrail\n",
        "    )\n",
        "    configs.append(\"Config 3: Ollama LLM + Local Guardrails\")\n",
        "\n",
        "# Configuration 4: OpenAI + Gemini Guardrails (if available)\n",
        "if openai_provider and gemini_guardrail:\n",
        "    agent_v4 = Agent(\n",
        "        config=simple_config,\n",
        "        llm_provider=openai_provider,\n",
        "        input_guardrail=gemini_guardrail,\n",
        "        output_guardrail=local_guardrail\n",
        "    )\n",
        "    configs.append(\"Config 4: OpenAI LLM + Gemini Input + Local Output Guardrails\")\n",
        "\n",
        "for config in configs:\n",
        "    print(f\"\u2713 {config}\")\n",
        "\n",
        "print(\"\\n All configurations use the same Agent interface!\")\n",
        "print(\"This demonstrates the power of abstraction and dependency injection.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Example 7: View Files Created by Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List files created by agents\n",
        "from pathlib import Path\n",
        "\n",
        "output_dir = Path(\"/tmp/agent_outputs\")\n",
        "\n",
        "if output_dir.exists():\n",
        "    files = list(output_dir.iterdir())\n",
        "    if files:\n",
        "        print(\"Files created by agents:\\n\")\n",
        "        for file in files:\n",
        "            print(f\"  \ud83d\udcc4 {file.name}\")\n",
        "            print(f\"     Size: {file.stat().st_size} bytes\")\n",
        "            print(f\"     Content preview:\")\n",
        "            with open(file, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "                preview = content[:200] + \"...\" if len(content) > 200 else content\n",
        "                print(f\"     {preview}\\n\")\n",
        "    else:\n",
        "        print(\"No files created yet. Run the examples above to create some!\")\n",
        "else:\n",
        "    print(\"Output directory doesn't exist yet. Run examples with file-saving tools first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates a production-ready implementation of agents with guardrails.\n",
        "\n",
        "### Key Design Principles\n",
        "\n",
        "1. **Abstraction**: Abstract base classes for LLM and Guardrail providers\n",
        "2. **Modularity**: Each component can be swapped independently\n",
        "3. **Extensibility**: Easy to add new providers, tools, and agents\n",
        "4. **Reusability**: SuperPrompts can be loaded and composed\n",
        "5. **Safety**: Input/Output guardrails with multiple provider options\n",
        "\n",
        "### Swappable Components\n",
        "\n",
        "- **LLM Providers**: OpenAI, Ollama, or implement your own\n",
        "- **Guardrail Providers**: OpenAI, Gemini, Local rules, or implement your own\n",
        "- **Tools**: File operations, custom tools via ToolRegistry\n",
        "- **Prompts**: Load from SuperPrompt library, compose multiple prompts\n",
        "\n",
        "### Use Cases Demonstrated\n",
        "\n",
        "1. \u2705 Creating agents from SuperPrompt templates\n",
        "2. \u2705 Multi-agent collaboration\n",
        "3. \u2705 Meta-agents creating other agents\n",
        "4. \u2705 Prompt composition and reuse\n",
        "5. \u2705 File-saving tools\n",
        "6. \u2705 Input/Output guardrails\n",
        "7. \u2705 Component swapping\n",
        "\n",
        "### Architecture Highlights\n",
        "\n",
        "**Example: Use Gemini for guardrails on OpenAI prompts**\n",
        "```python\n",
        "agent = Agent(\n",
        "    config=config,\n",
        "    llm_provider=OpenAIProvider(),      # OpenAI for generation\n",
        "    input_guardrail=GeminiGuardrailProvider(),  # Gemini for input checks\n",
        "    output_guardrail=LocalGuardrailProvider()   # Local for output checks\n",
        ")\n",
        "```\n",
        "\n",
        "This demonstrates true separation of concerns and dependency injection!\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Add more specialized tools (database access, API calls, etc.)\n",
        "- Implement agent memory/state persistence\n",
        "- Add more guardrail providers (e.g., Azure Content Safety)\n",
        "- Create domain-specific agent templates\n",
        "- Implement agent monitoring and logging\n",
        "- Add support for streaming responses\n",
        "- Create a web UI for agent interaction\n",
        "\n",
        "### References\n",
        "\n",
        "- [OpenAI Agents SDK Documentation](https://platform.openai.com/docs/guides/agents-sdk)\n",
        "- [LinkedIn Article on AI Agents with Guardrails](https://www.linkedin.com/posts/liord_ai-agents-are-powerful-but-they-need-activity-7306348810824241152-xSAx)\n",
        "- Implementation: `my_chat_gpt_utils/agents_sdk.py`"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
